{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflowで文書を自動でタグ分類\n",
    "\n",
    "doc2vec_test で作ったファイルを使い、新しい文書のタグを自動的に分類してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets.widgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from os import listdir, path\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "TRAIN_DATA_SIZE = 100\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 入力データを用意する\n",
    "\n",
    "作成したトレーニングデータとテストデータをそれぞれ読み込み、バッチで取得するための準備を行う\n",
    "\n",
    "`tf.train.shuffle_batch`はhttp://ykicisk.hatenablog.com/entry/2016/12/18/184840 が参考になった\n",
    "\n",
    "shuffle_batchを使わずbatchでやった場合、同じカテゴリがまとまって入力されるため、偏った結果になってしまった。\n",
    "shuffle_batchでランダムに取得することによって、確率的勾配降下法(SGD)になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': <tf.Tensor 'shuffle_batch:0' shape=(100, 100) dtype=float32>, 'id': <tf.Tensor 'shuffle_batch:1' shape=(100, 1) dtype=int64>, 'label': <tf.Tensor 'shuffle_batch:2' shape=(100, 1) dtype=int64>}\n",
      "{'feature': <tf.Tensor 'shuffle_batch_1:0' shape=(100, 100) dtype=float32>, 'id': <tf.Tensor 'shuffle_batch_1:1' shape=(100, 1) dtype=int64>, 'label': <tf.Tensor 'shuffle_batch_1:2' shape=(100, 1) dtype=int64>}\n"
     ]
    }
   ],
   "source": [
    "reader = tf.TFRecordReader()\n",
    "min_after_dequeue = 5000  # 5000個以上キューが貯まるまで待ってそこからランダムに取得をするような感じだと思われる\n",
    "capacity = min_after_dequeue + 3 * BATCH_SIZE\n",
    "\n",
    "\n",
    "# トレーニングデータの準備\n",
    "x_filename_queue = tf.train.string_input_producer(['../input/train.tfr'])\n",
    "_, x_serialized_example = reader.read(x_filename_queue)\n",
    "x_inputs = tf.parse_single_example(x_serialized_example, features={\n",
    "    'id': tf.FixedLenFeature([1], tf.int64),\n",
    "    'label': tf.FixedLenFeature([1], tf.int64),\n",
    "    'feature': tf.FixedLenFeature([BATCH_SIZE], tf.float32),\n",
    "})\n",
    "x_batch = tf.train.shuffle_batch(x_inputs, batch_size=BATCH_SIZE, capacity=capacity, min_after_dequeue=min_after_dequeue)\n",
    "print(x_batch)\n",
    "\n",
    "\n",
    "# テストデータの準備\n",
    "t_filename_queue = tf.train.string_input_producer(['../input/test.tfr'])\n",
    "_, t_serialized_example = reader.read(t_filename_queue)\n",
    "t_inputs = tf.parse_single_example(t_serialized_example, features={\n",
    "    'id': tf.FixedLenFeature([1], tf.int64),\n",
    "    'label': tf.FixedLenFeature([1], tf.int64),\n",
    "    'feature': tf.FixedLenFeature([BATCH_SIZE], tf.float32),\n",
    "})\n",
    "t_batch = tf.train.shuffle_batch(t_inputs, batch_size=BATCH_SIZE, capacity=capacity, min_after_dequeue=min_after_dequeue)\n",
    "print(t_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニングデータを入れる変数\n",
    "トレーニングデータの個数=None(無限)\n",
    "トレーニングデータの要素数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 100)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, TRAIN_DATA_SIZE])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重み\n",
    "トレーニングデータの要素数\n",
    "ラベルの種類数=9 (この場合のタグの種類は保存されているディレクトリの数)\n",
    "0で初期化する。\n",
    "重みは、トレーニングデータと学習アルゴリズムによって自動で獲得される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len([path.join('../data/text', x) for x in listdir('../data/text') if not x.endswith('.txt')]))\n",
    "\n",
    "W = tf.Variable(tf.zeros([TRAIN_DATA_SIZE, 9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バイアス\n",
    "ラベル数の数だけ0で初期化\n",
    "バイアスも、トレーニングデータと学習アルゴリズムによって自動で獲得される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = tf.Variable(tf.zeros([9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ソフトマックス関数\n",
    "ソフトマックス関数の出力は0から1の間の実数になり、出力の総和は1となる\n",
    "これにより文書が、各要素である確率を求めることができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正解ラベルデータ\n",
    "入力データとして取得した正解ラベルは0〜9の数字なので、9列のone hot配列に変換する\n",
    "one hotは正解となる要素の列だけに値が入り他は０になるやつなので、\n",
    "正解ラベルが`2`なら`[0, 5.0, 0, 0, 0, 0, 0, 0, 0]`みたいになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 9)\n"
     ]
    }
   ],
   "source": [
    "one_hot = tf.placeholder(tf.int32, [None])\n",
    "y_ = tf.one_hot(one_hot, depth=9, on_value = 5.0, off_value = 0.0, dtype=tf.float32)\n",
    "print(y_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交差エントロピー誤差\n",
    "損失関数\n",
    "正解ラベルとニューラルネットワークがどれだけ適合していないかを図るためのもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配降下法を用い交差エントロピー誤差が最小となるようyを最適化する\n",
    "0.01は学習率\n",
    "学習率はハイパーパラメータといって、この値によってモデル全体の結果が変わってくる大事な値。\n",
    "ハイパーパラメータは、重みやバイアスのように自動で獲得することはできない。\n",
    "\n",
    "TODO: 色々変えて試してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 学習させる\n",
    "100個のバッチを1000回実行してみる\n",
    "\n",
    "1エポック=`トレーニングデータをすべて使いきったときの回数`\n",
    "今回の場合7,400件程度のデータの9割ほどをトレーニングデータとして使っているので、その件数分を回したら1エポックということらしい\n",
    "\n",
    "HACK: 1エポックという単位が有るくらいだから、100エポックとか回したほうがいいのかな?\n",
    "TODO: accuracyの変遷を見てもあまり学習できてない気がする?いろいろ試したい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.68\n",
      "step 10, training accuracy 0.85\n",
      "step 20, training accuracy 0.82\n",
      "step 30, training accuracy 0.64\n",
      "step 40, training accuracy 0.75\n",
      "step 50, training accuracy 0.73\n",
      "step 60, training accuracy 0.81\n",
      "step 70, training accuracy 0.88\n",
      "step 80, training accuracy 0.79\n",
      "step 90, training accuracy 0.7\n",
      "step 100, training accuracy 0.69\n",
      "step 110, training accuracy 0.8\n",
      "step 120, training accuracy 0.79\n",
      "step 130, training accuracy 0.87\n",
      "step 140, training accuracy 0.78\n",
      "step 150, training accuracy 0.83\n",
      "step 160, training accuracy 0.66\n",
      "step 170, training accuracy 0.73\n",
      "step 180, training accuracy 0.7\n",
      "step 190, training accuracy 0.78\n",
      "step 200, training accuracy 0.77\n",
      "step 210, training accuracy 0.89\n",
      "step 220, training accuracy 0.67\n",
      "step 230, training accuracy 0.63\n",
      "step 240, training accuracy 0.77\n",
      "step 250, training accuracy 0.78\n",
      "step 260, training accuracy 0.83\n",
      "step 270, training accuracy 0.83\n",
      "step 280, training accuracy 0.86\n",
      "step 290, training accuracy 0.67\n",
      "step 300, training accuracy 0.69\n",
      "step 310, training accuracy 0.84\n",
      "step 320, training accuracy 0.87\n",
      "step 330, training accuracy 0.8\n",
      "step 340, training accuracy 0.83\n",
      "step 350, training accuracy 0.78\n",
      "step 360, training accuracy 0.69\n",
      "step 370, training accuracy 0.74\n",
      "step 380, training accuracy 0.76\n",
      "step 390, training accuracy 0.72\n",
      "step 400, training accuracy 0.82\n",
      "step 410, training accuracy 0.91\n",
      "step 420, training accuracy 0.75\n",
      "step 430, training accuracy 0.57\n",
      "step 440, training accuracy 0.77\n",
      "step 450, training accuracy 0.8\n",
      "step 460, training accuracy 0.79\n",
      "step 470, training accuracy 0.9\n",
      "step 480, training accuracy 0.76\n",
      "step 490, training accuracy 0.63\n",
      "step 500, training accuracy 0.79\n",
      "step 510, training accuracy 0.72\n",
      "step 520, training accuracy 0.83\n",
      "step 530, training accuracy 0.8\n",
      "step 540, training accuracy 0.8\n",
      "step 550, training accuracy 0.78\n",
      "step 560, training accuracy 0.56\n",
      "step 570, training accuracy 0.67\n",
      "step 580, training accuracy 0.85\n",
      "step 590, training accuracy 0.76\n",
      "step 600, training accuracy 0.82\n",
      "step 610, training accuracy 0.83\n",
      "step 620, training accuracy 0.73\n",
      "step 630, training accuracy 0.68\n",
      "step 640, training accuracy 0.77\n",
      "step 650, training accuracy 0.88\n",
      "step 660, training accuracy 0.82\n",
      "step 670, training accuracy 0.86\n",
      "step 680, training accuracy 0.72\n",
      "step 690, training accuracy 0.67\n",
      "step 700, training accuracy 0.7\n",
      "step 710, training accuracy 0.8\n",
      "step 720, training accuracy 0.81\n",
      "step 730, training accuracy 0.89\n",
      "step 740, training accuracy 0.82\n",
      "step 750, training accuracy 0.75\n",
      "step 760, training accuracy 0.67\n",
      "step 770, training accuracy 0.74\n",
      "step 780, training accuracy 0.72\n",
      "step 790, training accuracy 0.82\n",
      "step 800, training accuracy 0.78\n",
      "step 810, training accuracy 0.85\n",
      "step 820, training accuracy 0.66\n",
      "step 830, training accuracy 0.7\n",
      "step 840, training accuracy 0.73\n",
      "step 850, training accuracy 0.69\n",
      "step 860, training accuracy 0.88\n",
      "step 870, training accuracy 0.9\n",
      "step 880, training accuracy 0.87\n",
      "step 890, training accuracy 0.59\n",
      "step 900, training accuracy 0.74\n",
      "step 910, training accuracy 0.78\n",
      "step 920, training accuracy 0.79\n",
      "step 930, training accuracy 0.8\n",
      "step 940, training accuracy 0.89\n",
      "step 950, training accuracy 0.7\n",
      "step 960, training accuracy 0.69\n",
      "step 970, training accuracy 0.77\n",
      "step 980, training accuracy 0.84\n",
      "step 990, training accuracy 0.8\n"
     ]
    }
   ],
   "source": [
    "fp = FloatProgress(min=0, max=1000)\n",
    "display(fp)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "try:\n",
    "    for i in range(1000):\n",
    "        x_train = sess.run(x_batch)\n",
    "        sess.run(train_step, feed_dict={x: x_train['feature'], one_hot: x_train['label'].reshape((BATCH_SIZE))})\n",
    "        \n",
    "        # 10件ごとに予測と精度の計算をする\n",
    "        if i%10 == 0:\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={x: x_train['feature'], one_hot: x_train['label'].reshape((BATCH_SIZE))})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        fp.value = i\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Done training for %d steps.' % (step))\n",
    "finally:\n",
    "    coord.request_stop()\n",
    "    \n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストデータを使って確認してみる\n",
    "\n",
    "テストデータを500件取得して、それを使ってソフトマックス関数を実行してみる\n",
    "重みとバイアスが更新されているはずなので、いい結果が出る?\n",
    "\n",
    "TODO: 学習前に実行してみて、学習の効果を見てみたりしたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "結果\n",
      "test accuracy 0.85\n",
      "\n",
      "1件だけ見てみる\n",
      "id      : [290]\n",
      "label : [0]\n",
      "[  9.82768357e-01   3.50690279e-05   1.37721945e-05   1.44424368e-04\n",
      "   2.19040285e-05   2.19040285e-05   2.12749350e-04   5.95606631e-04\n",
      "   1.61861740e-02]\n",
      "label : 0\n"
     ]
    }
   ],
   "source": [
    "t_train = sess.run(t_batch)\n",
    "\n",
    "print(\"結果\")\n",
    "\n",
    "result = sess.run(y, feed_dict={x: t_train['feature']})\n",
    "\n",
    "t_correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "t_accuracy = tf.reduce_mean(tf.cast(t_correct_prediction, \"float\"))\n",
    "test_accuracy = sess.run(t_accuracy, feed_dict={x: t_train['feature'], one_hot: t_train['label'].reshape((BATCH_SIZE))})\n",
    "print(\"test accuracy %g\"%(test_accuracy))\n",
    "\n",
    "print(\"\\n1件だけ見てみる\")\n",
    "\n",
    "print(\"id      : {}\".format(t_train['id'][0]))\n",
    "print(\"label : {}\".format(t_train['label'][0]))\n",
    "\n",
    "print(result[0])\n",
    "print(\"label : {}\".format(np.argmax(result[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 結果を見て\n",
    "\n",
    "0.85という結果になりました。\n",
    "\n",
    "低め8割くらいは正しい分類ができるってことでしょうか。\n",
    "高く感じますが、5回1回失敗する機能と考えるとあまり使い物にならないですね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "6fc9e6b00b4e479a8bb8c54bd4ba09b5": {
     "views": [
      {
       "cell_index": 19
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
